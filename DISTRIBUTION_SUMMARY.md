# RÃ©sumÃ© de la Distribution - Phase 2 : Allocation

## âœ… Configuration SSH RÃ©ussie

La configuration SSH avec jump host a Ã©tÃ© mise en place avec succÃ¨s :
- **Bastion** : 193.55.163.204 (bsaidi@s-virtualserver7-lias)
- **Master** : 192.168.165.27 (ubuntu@master)
- **10 Workers** : 192.168.165.{101,138,80,89,126,249,194,46,233,63}

### Aliases SSH configurÃ©s
```bash
ssh pqdag-master      # Connexion au master
ssh pqdag-worker-1    # Connexion au worker 1
ssh pqdag-worker-2    # Connexion au worker 2
# ... jusqu'Ã  worker-10
```

Tous les workers sont accessibles sans mot de passe grÃ¢ce aux clÃ©s SSH configurÃ©es.

---

## âœ… Distribution ComplÃ¨te RÃ©ussie

### Statistiques de Distribution

**Dataset** : watdiv100k  
**Total fragments** : 918  
**Workers** : 10  

**Distribution Ã©quilibrÃ©e (METIS)** :
| Machine | Fragments | Taille Archive |
|---------|-----------|----------------|
| Worker 1 | 89 | 113 KB |
| Worker 2 | 93 | 130 KB |
| Worker 3 | 89 | 113 KB |
| Worker 4 | 94 | 253 KB |
| Worker 5 | 93 | 479 KB |
| Worker 6 | 89 | 133 KB |
| Worker 7 | 89 | 131 KB |
| Worker 8 | 94 | 183 KB |
| Worker 9 | 94 | 914 KB |
| Worker 10 | 94 | 225 KB |

### Pipeline d'Allocation Complet

#### Ã‰tape 1 : Calcul des statistiques (MPI)
```bash
mpiexec -n 4 python3 stat_MPI.py /app/storage/outputdata /app/storage/allocation_results/db
```
- âœ… GÃ©nÃ©ration de `db.stat` : 918 lignes, 1.1M
- âœ… Temps d'exÃ©cution : ~5.5 secondes avec 4 processus MPI

#### Ã‰tape 2 : GÃ©nÃ©ration du graphe de fragments
```bash
python3 generate_fragments_graph.py
```
- âœ… GÃ©nÃ©ration de `fragments_graph.quad` : 87,374 arÃªtes, 1.1M
- âœ… Format : "source predicate target weight"

#### Ã‰tape 3 : Allocation METIS
```bash
python3 weighted_metis.py
```
- âœ… GÃ©nÃ©ration de `affectation_weighted_metis.txt` : 918 allocations
- âœ… Distribution Ã©quilibrÃ©e : 89-94 fragments par machine

#### Ã‰tape 4 : Distribution aux workers
```bash
python3 distribute_fragments.py --config_file config_runtime.yaml
```
- âœ… CrÃ©ation de 10 archives tar.gz (une par worker)
- âœ… Transfert via SCP Ã  travers le jump host
- âœ… Extraction automatique sur chaque worker
- âœ… Chargement dans BTrees (`fragments_loader.py`)
- âœ… Nettoyage automatique des fichiers temporaires

---

## ğŸ“ Structure des Fichiers

### Sur le poste local
```
storage/
â”œâ”€â”€ outputdata/              # 918 fragments du dataset watdiv100k
â”‚   â”œâ”€â”€ *.data, *.dic, *.schema
â”‚   â”œâ”€â”€ spo_index.txt
â”‚   â”œâ”€â”€ ops_index.txt
â”‚   â””â”€â”€ predicates.txt
â”œâ”€â”€ allocation_results/      # RÃ©sultats de l'allocation
â”‚   â”œâ”€â”€ db.stat
â”‚   â”œâ”€â”€ fragments_graph.quad
â”‚   â””â”€â”€ affectation_weighted_metis.txt
â””â”€â”€ allocation_temp/         # Archives temporaires (nettoyÃ©es aprÃ¨s)
    â””â”€â”€ worker_*.tar.gz
```

### Sur chaque worker
```
/home/ubuntu/pqdag/
â”œâ”€â”€ data/                    # RÃ©pertoire temporaire (vide aprÃ¨s distribution)
â””â”€â”€ storage/                 # Stockage permanent des fragments
    â”œâ”€â”€ <fragment_id>        # Fichiers BTrees
    â”œâ”€â”€ <fragment_id>.schema
    â””â”€â”€ affectation          # Liste des fragments de ce worker
```

### Sur le master
```
/home/ubuntu/pqdag/data/
â”œâ”€â”€ spo_index.txt
â”œâ”€â”€ ops_index.txt
â”œâ”€â”€ predicates.txt
â””â”€â”€ global_affectation.txt   # Affectation globale
```

---

## ğŸ”§ Corrections EffectuÃ©es

### 1. Configuration SSH avec Jump Host
- âœ… Fichier `ssh_config_cluster` crÃ©Ã© avec ProxyJump
- âœ… Script `setup-ssh-cluster.sh` pour configuration automatique
- âœ… ClÃ©s SSH distribuÃ©es Ã  tous les nÅ“uds (1 bastion + 1 master + 10 workers)

### 2. Adaptation de `distribute_fragments.py`
- âœ… Utilisation des alias SSH (`pqdag-worker-X`) au lieu d'IPs directes
- âœ… Gestion automatique du jump host via configuration SSH
- âœ… Quotage correct des chemins avec espaces
- âœ… Gestion d'erreurs amÃ©liorÃ©e

### 3. Configuration Dynamique
- âœ… Template `config.yaml` avec variables `${WORKSPACE_ROOT}` et `${DATASET_NAME}`
- âœ… Script `generate_config.py` pour gÃ©nÃ©ration automatique
- âœ… Chemins adaptÃ©s au cluster : `/home/ubuntu/pqdag/data/` et `/home/ubuntu/pqdag/storage/`

### 4. Corrections de Bugs
- âœ… Bug path concatenation dans `stat_MPI.py` (lignes 39, 105)
- âœ… Ajout de pandas dans requirements.txt
- âœ… Correction du nom de fichier d'affectation (sans suffixe dataset)

---

## ğŸ¯ Prochaines Ã‰tapes

### Phase 2.5 : Backend API Allocation

CrÃ©er un endpoint Spring Boot pour l'allocation :

**Endpoint** : `POST /api/allocation/start`

**Request Body** :
```json
{
  "datasetName": "watdiv100k",
  "numMachines": 10,
  "cleanAfter": true
}
```

**FonctionnalitÃ©s** :
1. GÃ©nÃ©ration de `config_runtime.yaml`
2. ExÃ©cution de `stat_MPI.py` via Docker
3. ExÃ©cution de `generate_fragments_graph.py`
4. ExÃ©cution de `weighted_metis.py`
5. Retour des statistiques et rÃ©sultats

**Response** :
```json
{
  "status": "success",
  "statistics": {
    "totalFragments": 918,
    "totalEdges": 87374,
    "executionTime": "12.5s"
  },
  "distribution": [
    {"machine": 1, "fragments": 89},
    {"machine": 2, "fragments": 93},
    ...
  ]
}
```

### Phase 3 : Frontend Allocation GUI

**Component** : `AllocationComponent`

**Features** :
- Configuration : nombre de machines, dataset
- Gestion Master/Workers IPs
- Bouton "Start Allocation" avec loading state
- Indicateurs de progression (steps 1-4)
- Affichage des rÃ©sultats :
  - Graphique de distribution (bar chart)
  - Table de statistiques
  - Bouton de tÃ©lÃ©chargement du fichier d'affectation
- IntÃ©gration avec FragmentationComponent

### Phase 4 : Distribution GUI

**Endpoint** : `POST /api/allocation/distribute`

**Features** :
- Bouton "Distribute to Cluster"
- Validation SSH avant distribution
- Barre de progression par worker
- Logs en temps rÃ©el (WebSocket optionnel)
- VÃ©rification post-distribution

---

## ğŸ“Š MÃ©triques de Performance

### Allocation (local)
- **Ã‰tape 1 (MPI Statistics)** : ~5.5 secondes (4 processus)
- **Ã‰tape 2 (Graph Generation)** : ~2 secondes
- **Ã‰tape 3 (METIS Allocation)** : ~1 seconde
- **Total** : ~8.5 secondes pour 918 fragments

### Distribution (cluster via jump host)
- **CrÃ©ation des archives** : ~2 secondes (parallÃ¨le)
- **Transfert Worker 1** : ~1 seconde (113 KB)
- **Transfert Worker 9** : ~5 secondes (914 KB)
- **Chargement BTrees** : 1.7-3.1 secondes par worker
- **Total** : ~25-30 secondes pour 10 workers

---

## ğŸ”’ SÃ©curitÃ©

### SSH
- âœ… ClÃ©s SSH 4096 bits RSA
- âœ… Authentification par clÃ© (pas de mot de passe)
- âœ… Jump host (bastion) pour accÃ¨s cluster
- âœ… StrictHostKeyChecking dÃ©sactivÃ© pour le cluster privÃ©

### RÃ©seau
- âœ… Cluster sur rÃ©seau privÃ© (192.168.165.0/24)
- âœ… AccÃ¨s uniquement via bastion (193.55.163.204)
- âœ… Pas d'exposition directe des workers

---

## ğŸ“ Scripts de Test

### `test-allocation-simple.sh`
Teste les 3 premiÃ¨res Ã©tapes de l'allocation (sans distribution).

### `test-distribution.sh`
Teste la connectivitÃ© et la prÃ©paration sans vraie distribution.

### `test-distribution-real.sh`
**Vraie distribution** complÃ¨te vers le cluster (avec confirmation).

### ExÃ©cution
```bash
# Allocation seulement
./test-allocation-simple.sh

# Test de connectivitÃ©
./test-distribution.sh

# Distribution rÃ©elle
./test-distribution-real.sh
```

---

## âœ… Validation

Tous les tests ont rÃ©ussi :
- âœ… 10/10 workers accessibles
- âœ… Master accessible
- âœ… Archives crÃ©Ã©es et transfÃ©rÃ©es
- âœ… Fragments extraits sur chaque worker
- âœ… BTrees chargÃ©s avec succÃ¨s
- âœ… Distribution Ã©quilibrÃ©e validÃ©e (89-94 fragments/machine)

**Date de validation** : 16 dÃ©cembre 2024  
**Status** : Phase 2 (Allocation) âœ… COMPLÃˆTE
